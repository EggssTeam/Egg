# deepseek_local_model.py
from transformers import pipeline
import re
from datetime import datetime

# ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹è·¯å¾„ï¼ˆæŒ‡å‘snapshotsç›®å½•ï¼‰
MODEL_PATH = "D:/huggingface_cache/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562"


def load_model():
    """åŠ è½½æœ¬åœ°DeepSeekæ¨¡å‹"""
    try:
        pipe = pipeline(
            "text-generation",
            model=MODEL_PATH,
            device="cpu",  # ä½¿ç”¨CPUï¼ˆå¦‚éœ€GPUæ”¹ä¸º"cuda"ï¼‰
            trust_remote_code=True,
            max_length=1024
        )
        print("âœ… æœ¬åœ°DeepSeekæ¨¡å‹åŠ è½½æˆåŠŸ")
        return pipe
    except Exception as e:
        raise ValueError(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")


pipe = load_model()


def parse_generated_text(text: str):
    """
    è§£ææ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼Œæå–é—®é¢˜ã€é€‰é¡¹å’Œç­”æ¡ˆ
    è¿”å›æ ¼å¼: {
        "question": str,
        "options": {"A": str, "B": str, "C": str, "D": str},
        "correct_answer": str
    }
    """
    try:
        # æ¸…ç†è¾“å‡ºæ–‡æœ¬
        text = text.split("</think>")[-1].strip() if "</think>" in text else text

        # æå–å…³é”®éƒ¨åˆ†
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        if len(lines) < 7:
            raise ValueError("ç”Ÿæˆå†…å®¹æ ¼å¼ä¸ç¬¦åˆè¦æ±‚")

        return {
            "question": lines[0].replace("é—®é¢˜:", "").strip(),
            "options": {
                "A": lines[2].replace("A.", "").strip(),
                "B": lines[3].replace("B.", "").strip(),
                "C": lines[4].replace("C.", "").strip(),
                "D": lines[5].replace("D.", "").strip()
            },
            "correct_answer": lines[6].replace("æ­£ç¡®ç­”æ¡ˆ:", "").strip()
        }
    except Exception as e:
        print(f"âš ï¸ è§£æå¤±è´¥: {e}\nåŸå§‹æ–‡æœ¬:\n{text}")
        return None


def generate_questions_local(text: str, num_questions=1):
    """
    ä½¿ç”¨æœ¬åœ°DeepSeekæ¨¡å‹ç”Ÿæˆé€‰æ‹©é¢˜
    å‚æ•°:
        text: è¾“å…¥æ–‡æœ¬
        num_questions: ç”Ÿæˆçš„é—®é¢˜æ•°é‡
    è¿”å›:
        List[dict] æˆ–ç©ºåˆ—è¡¨ï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    if not text.strip():
        return []

    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šå‡ºé¢˜ä¸“å®¶ã€‚è¯·æ ¹æ®ä¸‹åˆ—å†…å®¹ç”Ÿæˆ{num_questions}é“å•é¡¹é€‰æ‹©é¢˜ã€‚

è¦æ±‚ï¼š
1. è€ƒå¯Ÿæ–‡æœ¬æ ¸å¿ƒçŸ¥è¯†ç‚¹
2. å››ä¸ªé€‰é¡¹ï¼ˆA-Dï¼‰æœ‰ä¸”ä»…ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆ
3. é”™è¯¯é€‰é¡¹è¦æœ‰è¿·æƒ‘æ€§

å†…å®¹ï¼š
{text}

è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå¾ªï¼‰ï¼š
é—®é¢˜: [é—®é¢˜å†…å®¹]
é€‰é¡¹:
A. [é€‰é¡¹A]
B. [é€‰é¡¹B]
C. [é€‰é¡¹C]
D. [é€‰é¡¹D]
æ­£ç¡®ç­”æ¡ˆ: [å­—æ¯]"""

    try:
        result = pipe(
            prompt,
            max_new_tokens=1024,
            do_sample=True,
            temperature=0.7
        )

        generated_text = result[0]['generated_text']
        # print(f"ğŸ”„ åŸå§‹ç”Ÿæˆå†…å®¹:\n{generated_text}")  # è°ƒè¯•ç”¨

        # è§£æç”Ÿæˆå†…å®¹
        questions = []
        parsed = parse_generated_text(generated_text)
        if parsed:
            questions.append(parsed)

        return questions if questions else []

    except Exception as e:
        print(f"âŒ ç”Ÿæˆé—®é¢˜æ—¶å‡ºé”™: {e}")
        return []